{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1b06b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from lightgbm import LGBMRegressor, early_stopping, log_evaluation\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.metrics import pairwise_distances, mean_absolute_error, r2_score\n",
    "from sklearn.linear_model import RidgeCV\n",
    "import optuna\n",
    "import warnings\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51471359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 5237980\n",
      "There is 200 unique stocks in the dataset.\n",
      "There is 481 unique dates in the dataset.\n",
      "The 'target' column has 88 NaN values.\n",
      "Null values per column:\n",
      "far_price                  2894342\n",
      "near_price                 2857180\n",
      "ask_price                      220\n",
      "imbalance_size                 220\n",
      "reference_price                220\n",
      "matched_size                   220\n",
      "wap                            220\n",
      "bid_price                      220\n",
      "target                          88\n",
      "time_id                          0\n",
      "ask_size                         0\n",
      "stock_id                         0\n",
      "bid_size                         0\n",
      "date_id                          0\n",
      "imbalance_buy_sell_flag          0\n",
      "seconds_in_bucket                0\n",
      "row_id                           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "print(f'Total rows: {len(df)}')\n",
    "print(f'There is {df.stock_id.nunique()} unique stocks in the dataset.')\n",
    "print(f'There is {df.date_id.nunique()} unique dates in the dataset.')\n",
    "\n",
    "nan_count = df['target'].isna().sum()\n",
    "print(f\"The 'target' column has {nan_count} NaN values.\")\n",
    "\n",
    "print('Null values per column:')\n",
    "print(df.isnull().sum().sort_values(ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fde1c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- Memory Optimization ---------------------- #\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    df = df.copy()\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if col_type.kind not in 'iuf':\n",
    "            continue\n",
    "        c_min, c_max = df[col].min(), df[col].max()\n",
    "        if col_type.kind == 'i':\n",
    "            if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                df[col] = df[col].astype(np.int8)\n",
    "            elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                df[col] = df[col].astype(np.int16)\n",
    "            elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                df[col] = df[col].astype(np.int32)\n",
    "        else:\n",
    "            if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                df[col] = df[col].astype(np.float32)\n",
    "            else:\n",
    "                df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    if verbose:\n",
    "        print(f'Memory reduced by {(start_mem - end_mem):.2f} MB ({100 * (start_mem - end_mem)/start_mem:.1f}%)')\n",
    "    return df\n",
    "\n",
    "# ---------------------- Feature Engineering ---------------------- #\n",
    "def feature_engineering(df):\n",
    "    df = df.copy()\n",
    "    df['spread'] = df['ask_price'] - df['bid_price']\n",
    "    df['mid_price'] = (df['ask_price'] + df['bid_price']) / 2\n",
    "    df['price_distance'] = df['reference_price'] - df['mid_price']\n",
    "    df['spread_ratio'] = df['spread'] / (df['mid_price'] + 1e-6)\n",
    "    df['wap_relative'] = (df['wap'] - df['reference_price']) / (df['reference_price'] + 1e-6)\n",
    "    df['liquidity_ratio'] = df['matched_size'] / (df['matched_size'] + df['imbalance_size'] + 1e-6)\n",
    "    df['bid_pressure'] = df['bid_size'] / (df['bid_size'] + df['ask_size'] + 1e-6)\n",
    "    df['volume'] = df['ask_size'] + df['bid_size']\n",
    "    df['size_imbalance'] = df['bid_size'] / (df['ask_size'] + 1e-6)\n",
    "    df['total_imbalance'] = df['imbalance_size'] * df['imbalance_buy_sell_flag']\n",
    "    df['price_momentum'] = df['reference_price'] / (df['wap'] + 1e-6)\n",
    "    df['market_heat'] = df['volume'] / (df['matched_size'] + 1e-6)\n",
    "    df['far_price'] = df['far_price'].fillna(df['reference_price'])\n",
    "    df['near_price'] = df['near_price'].fillna(df['reference_price'])\n",
    "    df['auction_efficiency'] = (df['far_price'] - df['near_price']) / (df['reference_price'] + 1e-6)\n",
    "    df['price_discovery'] = (df['reference_price'] - df['near_price']) / (df['reference_price'] + 1e-6)\n",
    "    return df\n",
    "\n",
    "# ---------------------- Time Features ---------------------- #\n",
    "def create_time_features(df):\n",
    "    df = df.copy()\n",
    "    df['time_to_close'] = 600 - df['seconds_in_bucket']\n",
    "    df['is_early_auction'] = (df['seconds_in_bucket'] < 300).astype(np.int8)\n",
    "    df['is_late_auction'] = (df['seconds_in_bucket'] >= 300).astype(np.int8)\n",
    "    df['seconds_sin'] = np.sin(2 * np.pi * df['seconds_in_bucket'] / 600)\n",
    "    df['seconds_cos'] = np.cos(2 * np.pi * df['seconds_in_bucket'] / 600)\n",
    "    return df\n",
    "\n",
    "# ---------------------- Stock-Level Aggregations ---------------------- #\n",
    "def compute_stock_level_stats(train_df):\n",
    "    \"\"\"Compute and store stock-level statistics from the training data.\"\"\"\n",
    "    cols = ['wap', 'imbalance_size', 'matched_size', 'volume', 'spread']\n",
    "    stock_stats = train_df.groupby('stock_id', observed=True)[cols].agg(['mean', 'std'])\n",
    "    stock_stats.columns = ['_'.join(c) for c in stock_stats.columns]\n",
    "    stock_stats = stock_stats.fillna(0)\n",
    "    return stock_stats\n",
    "\n",
    "def apply_stock_level_features(df, stock_stats):\n",
    "    \"\"\"Use precomputed stock-level stats (from train) on new data.\"\"\"\n",
    "    df = df.merge(stock_stats, on='stock_id', how='left')\n",
    "    for col in ['wap', 'imbalance_size', 'matched_size', 'volume', 'spread']:\n",
    "        df[f'{col}_stock_normalized'] = (\n",
    "            df[col] - df[f'{col}_mean']\n",
    "        ) / (df[f'{col}_std'] + 1e-6)\n",
    "    return df\n",
    "\n",
    "# ---------------------- Market-Wide Features ---------------------- #\n",
    "def create_market_wide_features(df):\n",
    "    df = df.copy()\n",
    "    market_stats = df.groupby(['date_id', 'seconds_in_bucket'], observed=True).agg({\n",
    "        'wap': ['mean', 'std', 'median'],\n",
    "        'volume': ['mean', 'sum'],\n",
    "        'imbalance_size': ['mean', 'sum'],\n",
    "        'bid_size': 'sum',\n",
    "        'ask_size': 'sum',\n",
    "        'spread': 'mean'\n",
    "    }).reset_index()\n",
    "    market_stats.columns = [\n",
    "        'date_id', 'seconds_in_bucket',\n",
    "        'market_wap_mean', 'market_wap_std', 'market_wap_median',\n",
    "        'market_volume_mean', 'market_volume_total',\n",
    "        'market_imbalance_mean', 'market_imbalance_total',\n",
    "        'market_bid_total', 'market_ask_total', 'market_spread_mean'\n",
    "    ]\n",
    "    df = df.merge(market_stats, on=['date_id', 'seconds_in_bucket'], how='left')\n",
    "    df['market_sentiment'] = df['market_bid_total'] / (df['market_bid_total'] + df['market_ask_total'] + 1e-6)\n",
    "    df['market_imbalance_ratio'] = df['market_imbalance_total'] / (df['market_volume_total'] + 1e-6)\n",
    "    df['market_volatility'] = df['market_wap_std'] / (df['market_wap_mean'] + 1e-6)\n",
    "    df['wap_vs_market'] = df['wap'] / (df['market_wap_mean'] + 1e-6)\n",
    "    df['volume_vs_market'] = df['volume'] / (df['market_volume_mean'] + 1e-6)\n",
    "    return df\n",
    "\n",
    "# ---------------------- Lag Features ---------------------- #\n",
    "def create_same_day_lag_features(df, lag_steps=[1,3,6]):\n",
    "    df = df.sort_values(['stock_id', 'date_id', 'seconds_in_bucket']).reset_index(drop=True)\n",
    "    grouped = df.groupby(['stock_id','date_id'], observed=True)\n",
    "    lag_cols = ['wap', 'wap_relative', 'liquidity_ratio', 'bid_pressure',\n",
    "                'imbalance_size', 'matched_size', 'volume', 'spread']\n",
    "    eps = 1e-9\n",
    "    for col in lag_cols:\n",
    "        for step in lag_steps:\n",
    "            lag_col = f\"{col}_lag_{step}\"\n",
    "            prev = grouped[col].shift(step)\n",
    "            df[lag_col] = (df[col] - prev) / (prev + eps)\n",
    "            df[lag_col] = df[lag_col].fillna(0)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------- Extra Features ---------------------- #\n",
    "def extra_features(df):\n",
    "    df = df.copy()\n",
    "    eps = 1e-9\n",
    "    # immediate changes\n",
    "    df = df.sort_values(['stock_id', 'date_id', 'seconds_in_bucket'])\n",
    "    df['wap_prev'] = df.groupby(['stock_id','date_id'])['wap'].shift(1)\n",
    "    df['wap_diff'] = (df['wap'] - df['wap_prev']).fillna(0)\n",
    "    df['wap_ret'] = df['wap_diff'] / (df['wap_prev'] + eps)\n",
    "    # near/far gaps\n",
    "    df['near_far_gap'] = df['far_price'] - df['near_price']\n",
    "    df['near_gap_rel'] = (df['near_price'] - df['reference_price']) / (df['reference_price'] + eps)\n",
    "    df['far_gap_rel'] = (df['far_price'] - df['reference_price']) / (df['reference_price'] + eps)\n",
    "    # logs\n",
    "    df['log_matched'] = np.log1p(df['matched_size'])\n",
    "    df['log_imbalance'] = np.log1p(np.abs(df['imbalance_size']))\n",
    "    # signed imbalance already present as total_imbalance\n",
    "    # rank within timestamp\n",
    "    df['wap_rank'] = df.groupby(['date_id','seconds_in_bucket'])['wap'].rank(pct=True)\n",
    "    df['spread_rank'] = df.groupby(['date_id','seconds_in_bucket'])['spread'].rank(pct=True)\n",
    "    df['liquidity_rank'] = df.groupby(['date_id','seconds_in_bucket'])['matched_size'].rank(pct=True)\n",
    "    # simple EWMA within day (past only)\n",
    "    df['wap_ewm_3'] = df.groupby(['stock_id','date_id'])['wap'].apply(lambda x: x.shift(1).ewm(span=3, adjust=False).mean()).reset_index(level=[0,1], drop=True)\n",
    "    df['wap_ewm_5'] = df.groupby(['stock_id','date_id'])['wap'].apply(lambda x: x.shift(1).ewm(span=5, adjust=False).mean()).reset_index(level=[0,1], drop=True)\n",
    "    df.fillna(0, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------- Pipeline ---------------------- #\n",
    "def process_dataset(train_df, test_df=None):\n",
    "    print(\"▶ Step 1: Basic feature engineering...\")\n",
    "    train_df = feature_engineering(train_df)\n",
    "    if test_df is not None:\n",
    "        test_df = feature_engineering(test_df)\n",
    "\n",
    "    print(\"▶ Step 2: Time features...\")\n",
    "    train_df = create_time_features(train_df)\n",
    "    if test_df is not None:\n",
    "        test_df = create_time_features(test_df)\n",
    "\n",
    "    print(\"▶ Step 3: Compute stock-level features from train and apply to all...\")\n",
    "    stock_stats = compute_stock_level_stats(train_df)\n",
    "    train_df = apply_stock_level_features(train_df, stock_stats)\n",
    "    if test_df is not None:\n",
    "        test_df = apply_stock_level_features(test_df, stock_stats)\n",
    "\n",
    "    print(\"▶ Step 4: Market-wide features...\")\n",
    "    train_df = create_market_wide_features(train_df)\n",
    "    if test_df is not None:\n",
    "        test_df = create_market_wide_features(test_df)\n",
    "\n",
    "    print(\"▶ Step 5: Lag features...\")\n",
    "    train_df = create_same_day_lag_features(train_df)\n",
    "    if test_df is not None:\n",
    "        test_df = create_same_day_lag_features(test_df)\n",
    "\n",
    "    print(\"▶ Step 6: Extra features...\")\n",
    "    train_df = extra_features(train_df)\n",
    "    if test_df is not None:\n",
    "        test_df = extra_features(test_df)\n",
    "\n",
    "\n",
    "    # Handle missing values\n",
    "\n",
    "    num_cols = train_df.select_dtypes(include=[np.number]).columns\n",
    "    num_cols = [c for c in num_cols if c != 'target']\n",
    "\n",
    "    # Fill missing values in train and test using train medians\n",
    "    train_df[num_cols] = train_df[num_cols].fillna(train_df[num_cols].median())\n",
    "    if test_df is not None:\n",
    "        common_cols = [c for c in num_cols if c in test_df.columns]\n",
    "        test_df[common_cols] = test_df[common_cols].fillna(train_df[common_cols].median())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"✅ Train shape: {train_df.shape}\")\n",
    "    if test_df is not None:\n",
    "        print(f\"✅ Test shape: {test_df.shape}\")\n",
    "\n",
    "    return train_df, test_df, stock_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de071dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial train shape: (5237980, 17)\n",
      "Memory reduced by 374.65 MB (55.1%)\n",
      "Reduced train shape: (5237980, 17)\n",
      "Reduced train shape: (4709980, 17) Reduced validation shape: (528000, 17)\n",
      "▶ Step 1: Basic feature engineering...\n",
      "▶ Step 2: Time features...\n",
      "▶ Step 3: Compute stock-level features from train and apply to all...\n",
      "▶ Step 4: Market-wide features...\n",
      "▶ Step 5: Lag features...\n",
      "▶ Step 6: Extra features...\n",
      "✅ Train shape: (4709980, 103)\n",
      "✅ Test shape: (528000, 103)\n"
     ]
    }
   ],
   "source": [
    "def temporal_train_val_split(df, val_frac=0.1, date_col='date_id', random_state=42):\n",
    "    dates = np.sort(df[date_col].unique())\n",
    "    n_val = max(1, int(len(dates) * val_frac))\n",
    "    val_dates = dates[-n_val:]\n",
    "    train_dates = dates[:-n_val]\n",
    "    train_df = df[df[date_col].isin(train_dates)].reset_index(drop=True)\n",
    "    val_df = df[df[date_col].isin(val_dates)].reset_index(drop=True)\n",
    "    return train_df, val_df\n",
    "\n",
    "\n",
    "train_df_full = pd.read_csv(\"train.csv\")\n",
    "print(\"Initial train shape:\", train_df_full.shape)\n",
    "\n",
    "train_df_full = reduce_mem_usage(train_df_full)\n",
    "print(\"Reduced train shape:\", train_df_full.shape)\n",
    "\n",
    "# ---- STEP 5: Train/Validation Split ----\n",
    "train_df, val_df = temporal_train_val_split(train_df_full, val_frac=0.1)\n",
    "print(\"Reduced train shape:\", train_df.shape, \"Reduced validation shape:\", val_df.shape)\n",
    "\n",
    "\n",
    "train_df, val_df, stock_stats = process_dataset(train_df, val_df)\n",
    "\n",
    "target_col = 'target'\n",
    "feature_cols = [c for c in train_df.columns if c not in ['target', 'date_id', 'time_id', 'stock_id','row_id']]\n",
    "\n",
    "X_train, y_train = train_df[feature_cols], train_df[target_col]\n",
    "X_val, y_val = val_df[feature_cols], val_df[target_col]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca328474",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-20 11:37:16,063] A new study created in memory with name: no-name-e5b5e80f-9243-4250-8e64-e25e51430053\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a511c44bfbc4012a3553380e17e873d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9294568657172472, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9294568657172472\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7128491685751819, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7128491685751819\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9294568657172472, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9294568657172472\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7128491685751819, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7128491685751819\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.157580 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 22870\n",
      "[LightGBM] [Info] Number of data points in the train set: 4709980, number of used features: 98\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9294568657172472, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9294568657172472\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7128491685751819, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7128491685751819\n",
      "[LightGBM] [Info] Start training from score -0.069737\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[381]\tvalid_0's l1: 5.84391\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9294568657172472, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9294568657172472\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7128491685751819, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7128491685751819\n",
      "[I 2025-10-20 11:38:18,262] Trial 0 finished with value: 5.84390949766734 and parameters: {'learning_rate': 0.08349061938034219, 'num_leaves': 66, 'max_depth': 10, 'feature_fraction': 0.9294568657172472, 'bagging_fraction': 0.7128491685751819}. Best is trial 0 with value: 5.84390949766734.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9040935248906057, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9040935248906057\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8883272308679079, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8883272308679079\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9040935248906057, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9040935248906057\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8883272308679079, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8883272308679079\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.156988 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 22870\n",
      "[LightGBM] [Info] Number of data points in the train set: 4709980, number of used features: 98\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9040935248906057, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9040935248906057\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8883272308679079, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8883272308679079\n",
      "[LightGBM] [Info] Start training from score -0.069737\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[400]\tvalid_0's l1: 5.84661\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9040935248906057, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9040935248906057\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8883272308679079, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8883272308679079\n",
      "[I 2025-10-20 11:39:40,011] Trial 1 finished with value: 5.846613873388579 and parameters: {'learning_rate': 0.024673869393414833, 'num_leaves': 69, 'max_depth': 9, 'feature_fraction': 0.9040935248906057, 'bagging_fraction': 0.8883272308679079}. Best is trial 0 with value: 5.84390949766734.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8720149192300248, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8720149192300248\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7426287069892578, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7426287069892578\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8720149192300248, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8720149192300248\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7426287069892578, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7426287069892578\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.153369 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 22870\n",
      "[LightGBM] [Info] Number of data points in the train set: 4709980, number of used features: 98\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8720149192300248, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8720149192300248\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7426287069892578, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7426287069892578\n",
      "[LightGBM] [Info] Start training from score -0.069737\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[400]\tvalid_0's l1: 5.85109\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8720149192300248, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8720149192300248\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7426287069892578, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7426287069892578\n",
      "[I 2025-10-20 11:41:20,854] Trial 2 finished with value: 5.851092730223819 and parameters: {'learning_rate': 0.014146648853932098, 'num_leaves': 114, 'max_depth': 9, 'feature_fraction': 0.8720149192300248, 'bagging_fraction': 0.7426287069892578}. Best is trial 0 with value: 5.84390949766734.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8270080503189581, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8270080503189581\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8922704949060148, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8922704949060148\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8270080503189581, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8270080503189581\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8922704949060148, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8922704949060148\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.158252 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 22870\n",
      "[LightGBM] [Info] Number of data points in the train set: 4709980, number of used features: 98\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8270080503189581, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8270080503189581\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8922704949060148, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8922704949060148\n",
      "[LightGBM] [Info] Start training from score -0.069737\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[271]\tvalid_0's l1: 5.84169\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8270080503189581, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8270080503189581\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8922704949060148, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8922704949060148\n",
      "[I 2025-10-20 11:42:24,607] Trial 3 finished with value: 5.841692000974178 and parameters: {'learning_rate': 0.08778848545464664, 'num_leaves': 114, 'max_depth': 11, 'feature_fraction': 0.8270080503189581, 'bagging_fraction': 0.8922704949060148}. Best is trial 3 with value: 5.841692000974178.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7490886123459987, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7490886123459987\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7125547705494246, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7125547705494246\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7490886123459987, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7490886123459987\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7125547705494246, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7125547705494246\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.170590 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 22870\n",
      "[LightGBM] [Info] Number of data points in the train set: 4709980, number of used features: 98\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7490886123459987, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7490886123459987\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7125547705494246, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7125547705494246\n",
      "[LightGBM] [Info] Start training from score -0.069737\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[399]\tvalid_0's l1: 5.84125\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7490886123459987, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7490886123459987\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7125547705494246, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7125547705494246\n",
      "[I 2025-10-20 11:43:43,889] Trial 4 finished with value: 5.84125145534831 and parameters: {'learning_rate': 0.05374395283707952, 'num_leaves': 74, 'max_depth': 11, 'feature_fraction': 0.7490886123459987, 'bagging_fraction': 0.7125547705494246}. Best is trial 4 with value: 5.84125145534831.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6575736860124717, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6575736860124717\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6390947308406322, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6390947308406322\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6575736860124717, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6575736860124717\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6390947308406322, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6390947308406322\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.159954 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 22870\n",
      "[LightGBM] [Info] Number of data points in the train set: 4709980, number of used features: 98\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6575736860124717, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6575736860124717\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6390947308406322, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6390947308406322\n",
      "[LightGBM] [Info] Start training from score -0.069737\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[399]\tvalid_0's l1: 5.84479\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6575736860124717, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6575736860124717\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6390947308406322, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6390947308406322\n",
      "[I 2025-10-20 11:45:10,036] Trial 5 finished with value: 5.844792294162836 and parameters: {'learning_rate': 0.035619697811223316, 'num_leaves': 71, 'max_depth': 9, 'feature_fraction': 0.6575736860124717, 'bagging_fraction': 0.6390947308406322}. Best is trial 4 with value: 5.84125145534831.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7952638267578118, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7952638267578118\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6623990679080346, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6623990679080346\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7952638267578118, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7952638267578118\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6623990679080346, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6623990679080346\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.156840 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 22870\n",
      "[LightGBM] [Info] Number of data points in the train set: 4709980, number of used features: 98\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7952638267578118, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7952638267578118\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6623990679080346, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6623990679080346\n",
      "[LightGBM] [Info] Start training from score -0.069737\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[242]\tvalid_0's l1: 5.84284\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7952638267578118, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7952638267578118\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6623990679080346, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6623990679080346\n",
      "[I 2025-10-20 11:46:09,636] Trial 6 finished with value: 5.842840491568965 and parameters: {'learning_rate': 0.09909410963143199, 'num_leaves': 111, 'max_depth': 11, 'feature_fraction': 0.7952638267578118, 'bagging_fraction': 0.6623990679080346}. Best is trial 4 with value: 5.84125145534831.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7001830813866442, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7001830813866442\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6730626940418036, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6730626940418036\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7001830813866442, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7001830813866442\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6730626940418036, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6730626940418036\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.157339 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 22870\n",
      "[LightGBM] [Info] Number of data points in the train set: 4709980, number of used features: 98\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7001830813866442, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7001830813866442\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6730626940418036, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6730626940418036\n",
      "[LightGBM] [Info] Start training from score -0.069737\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[400]\tvalid_0's l1: 5.8496\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7001830813866442, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7001830813866442\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6730626940418036, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6730626940418036\n",
      "[I 2025-10-20 11:47:51,383] Trial 7 finished with value: 5.849599604421125 and parameters: {'learning_rate': 0.017293927432108266, 'num_leaves': 90, 'max_depth': 9, 'feature_fraction': 0.7001830813866442, 'bagging_fraction': 0.6730626940418036}. Best is trial 4 with value: 5.84125145534831.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9845317898189788, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9845317898189788\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9084386338167187, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9084386338167187\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9845317898189788, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9845317898189788\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9084386338167187, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9084386338167187\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.173990 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 22870\n",
      "[LightGBM] [Info] Number of data points in the train set: 4709980, number of used features: 98\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9845317898189788, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9845317898189788\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9084386338167187, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9084386338167187\n",
      "[LightGBM] [Info] Start training from score -0.069737\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[312]\tvalid_0's l1: 5.84029\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9845317898189788, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9845317898189788\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9084386338167187, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9084386338167187\n",
      "[I 2025-10-20 11:49:10,581] Trial 8 finished with value: 5.8402943273432975 and parameters: {'learning_rate': 0.06119852025484961, 'num_leaves': 115, 'max_depth': 9, 'feature_fraction': 0.9845317898189788, 'bagging_fraction': 0.9084386338167187}. Best is trial 8 with value: 5.8402943273432975.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7049479465682039, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7049479465682039\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7993435518347413, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7993435518347413\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7049479465682039, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7049479465682039\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7993435518347413, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7993435518347413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.142881 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 22870\n",
      "[LightGBM] [Info] Number of data points in the train set: 4709980, number of used features: 98\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7049479465682039, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7049479465682039\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7993435518347413, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7993435518347413\n",
      "[LightGBM] [Info] Start training from score -0.069737\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[166]\tvalid_0's l1: 5.8437\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7049479465682039, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7049479465682039\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7993435518347413, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7993435518347413\n",
      "[I 2025-10-20 11:49:59,483] Trial 9 finished with value: 5.843702244622044 and parameters: {'learning_rate': 0.08869729796650307, 'num_leaves': 94, 'max_depth': 9, 'feature_fraction': 0.7049479465682039, 'bagging_fraction': 0.7993435518347413}. Best is trial 8 with value: 5.8402943273432975.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9828305533391749, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9828305533391749\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9955264283430567, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9955264283430567\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9828305533391749, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9828305533391749\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9955264283430567, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9955264283430567\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.168764 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 22870\n",
      "[LightGBM] [Info] Number of data points in the train set: 4709980, number of used features: 98\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9828305533391749, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9828305533391749\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9955264283430567, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9955264283430567\n",
      "[LightGBM] [Info] Start training from score -0.069737\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[380]\tvalid_0's l1: 5.84545\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9828305533391749, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9828305533391749\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9955264283430567, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9955264283430567\n",
      "[I 2025-10-20 11:51:12,666] Trial 10 finished with value: 5.845452512260275 and parameters: {'learning_rate': 0.06197124191476038, 'num_leaves': 49, 'max_depth': 10, 'feature_fraction': 0.9828305533391749, 'bagging_fraction': 0.9955264283430567}. Best is trial 8 with value: 5.8402943273432975.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.768090267319851, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.768090267319851\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.88097954165297, subsample=1.0 will be ignored. Current value: bagging_fraction=0.88097954165297\n",
      "[LightGBM] [Warning] feature_fraction is set=0.768090267319851, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.768090267319851\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.88097954165297, subsample=1.0 will be ignored. Current value: bagging_fraction=0.88097954165297\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.146615 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 22870\n",
      "[LightGBM] [Info] Number of data points in the train set: 4709980, number of used features: 98\n",
      "[LightGBM] [Warning] feature_fraction is set=0.768090267319851, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.768090267319851\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.88097954165297, subsample=1.0 will be ignored. Current value: bagging_fraction=0.88097954165297\n",
      "[LightGBM] [Info] Start training from score -0.069737\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[396]\tvalid_0's l1: 5.848\n",
      "[LightGBM] [Warning] feature_fraction is set=0.768090267319851, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.768090267319851\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.88097954165297, subsample=1.0 will be ignored. Current value: bagging_fraction=0.88097954165297\n",
      "[I 2025-10-20 11:52:23,445] Trial 11 finished with value: 5.848002103677131 and parameters: {'learning_rate': 0.05726354989349297, 'num_leaves': 34, 'max_depth': 11, 'feature_fraction': 0.768090267319851, 'bagging_fraction': 0.88097954165297}. Best is trial 8 with value: 5.8402943273432975.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7548442402753904, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7548442402753904\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9987506383338034, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9987506383338034\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7548442402753904, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7548442402753904\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9987506383338034, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9987506383338034\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.149067 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 22870\n",
      "[LightGBM] [Info] Number of data points in the train set: 4709980, number of used features: 98\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7548442402753904, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7548442402753904\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9987506383338034, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9987506383338034\n",
      "[LightGBM] [Info] Start training from score -0.069737\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[399]\tvalid_0's l1: 5.8415\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7548442402753904, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7548442402753904\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9987506383338034, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9987506383338034\n",
      "[I 2025-10-20 11:53:52,425] Trial 12 finished with value: 5.841501471450833 and parameters: {'learning_rate': 0.04289475319889975, 'num_leaves': 92, 'max_depth': 10, 'feature_fraction': 0.7548442402753904, 'bagging_fraction': 0.9987506383338034}. Best is trial 8 with value: 5.8402943273432975.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9913602790124548, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9913602790124548\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8088496206051371, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8088496206051371\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9913602790124548, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9913602790124548\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8088496206051371, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8088496206051371\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.169288 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 22870\n",
      "[LightGBM] [Info] Number of data points in the train set: 4709980, number of used features: 98\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9913602790124548, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9913602790124548\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8088496206051371, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8088496206051371\n",
      "[LightGBM] [Info] Start training from score -0.069737\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[247]\tvalid_0's l1: 5.84115\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9913602790124548, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9913602790124548\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8088496206051371, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8088496206051371\n",
      "[I 2025-10-20 11:55:02,018] Trial 13 finished with value: 5.841151325876809 and parameters: {'learning_rate': 0.069894445044483, 'num_leaves': 128, 'max_depth': 11, 'feature_fraction': 0.9913602790124548, 'bagging_fraction': 0.8088496206051371}. Best is trial 8 with value: 5.8402943273432975.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.986385487639305, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.986385487639305\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.810473184318834, subsample=1.0 will be ignored. Current value: bagging_fraction=0.810473184318834\n",
      "[LightGBM] [Warning] feature_fraction is set=0.986385487639305, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.986385487639305\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.810473184318834, subsample=1.0 will be ignored. Current value: bagging_fraction=0.810473184318834\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.169319 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 22870\n",
      "[LightGBM] [Info] Number of data points in the train set: 4709980, number of used features: 98\n",
      "[LightGBM] [Warning] feature_fraction is set=0.986385487639305, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.986385487639305\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.810473184318834, subsample=1.0 will be ignored. Current value: bagging_fraction=0.810473184318834\n",
      "[LightGBM] [Info] Start training from score -0.069737\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[395]\tvalid_0's l1: 5.83864\n",
      "[LightGBM] [Warning] feature_fraction is set=0.986385487639305, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.986385487639305\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.810473184318834, subsample=1.0 will be ignored. Current value: bagging_fraction=0.810473184318834\n",
      "[I 2025-10-20 11:56:29,473] Trial 14 finished with value: 5.838637325713208 and parameters: {'learning_rate': 0.0691763254937426, 'num_leaves': 128, 'max_depth': 10, 'feature_fraction': 0.986385487639305, 'bagging_fraction': 0.810473184318834}. Best is trial 14 with value: 5.838637325713208.\n",
      "Best LGB params: {'learning_rate': 0.0691763254937426, 'num_leaves': 128, 'max_depth': 10, 'feature_fraction': 0.986385487639305, 'bagging_fraction': 0.810473184318834}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def objective_lgb(trial):\n",
    "    params = {\n",
    "        'n_estimators': 400,\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 31, 128),\n",
    "        'max_depth': trial.suggest_int('max_depth', 9, 11),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.6, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.6, 1.0),\n",
    "        'random_state': 42,\n",
    "        'objective': 'regression_l1',\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "\n",
    "    model = LGBMRegressor(**params)\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        eval_metric='mae',\n",
    "        callbacks=[early_stopping(50), log_evaluation(0)]\n",
    "    )\n",
    "\n",
    "    preds = model.predict(X_val)\n",
    "    return mean_absolute_error(y_val, preds)\n",
    "\n",
    "study_lgb = optuna.create_study(direction='minimize')\n",
    "study_lgb.optimize(objective_lgb, n_trials=15, show_progress_bar=True)\n",
    "print(\"Best LGB params:\", study_lgb.best_trial.params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d734c8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-20 11:56:29,484] A new study created in memory with name: no-name-174164e0-38ef-4125-8167-49df4c1ecb4c\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5c77686b44e454db252ab84d2dccd09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-20 11:57:37,170] Trial 0 finished with value: 5.876413345336914 and parameters: {'learning_rate': 0.09257685584930038, 'max_depth': 6, 'subsample': 0.9024186936422618, 'colsample_bytree': 0.9139275324869054, 'reg_lambda': 0.13738695701265477}. Best is trial 0 with value: 5.876413345336914.\n",
      "[I 2025-10-20 11:59:02,125] Trial 1 finished with value: 5.869625568389893 and parameters: {'learning_rate': 0.0621146686645197, 'max_depth': 8, 'subsample': 0.7870470096841863, 'colsample_bytree': 0.8043740315844775, 'reg_lambda': 0.139712756884719}. Best is trial 1 with value: 5.869625568389893.\n",
      "[I 2025-10-20 12:00:11,012] Trial 2 finished with value: 5.86167049407959 and parameters: {'learning_rate': 0.04709627535217298, 'max_depth': 6, 'subsample': 0.9968225734413377, 'colsample_bytree': 0.8183125092750367, 'reg_lambda': 0.28568992493391965}. Best is trial 2 with value: 5.86167049407959.\n",
      "[I 2025-10-20 12:01:39,033] Trial 3 finished with value: 5.85768985748291 and parameters: {'learning_rate': 0.03667514229823883, 'max_depth': 8, 'subsample': 0.9522645580022315, 'colsample_bytree': 0.9030693986054679, 'reg_lambda': 1.3468587149554854}. Best is trial 3 with value: 5.85768985748291.\n",
      "[I 2025-10-20 12:02:48,696] Trial 4 finished with value: 5.867441177368164 and parameters: {'learning_rate': 0.06259352017850611, 'max_depth': 6, 'subsample': 0.8990530540102747, 'colsample_bytree': 0.8381663873823174, 'reg_lambda': 1.4873905676913532}. Best is trial 3 with value: 5.85768985748291.\n",
      "[I 2025-10-20 12:04:15,267] Trial 5 finished with value: 5.860657215118408 and parameters: {'learning_rate': 0.04843766868282912, 'max_depth': 8, 'subsample': 0.6936862763583571, 'colsample_bytree': 0.8995927014121388, 'reg_lambda': 6.361265802583006}. Best is trial 3 with value: 5.85768985748291.\n",
      "[I 2025-10-20 12:05:28,848] Trial 6 finished with value: 5.871265888214111 and parameters: {'learning_rate': 0.06825220749654043, 'max_depth': 6, 'subsample': 0.6586813611988023, 'colsample_bytree': 0.842211185319226, 'reg_lambda': 0.030976389276711005}. Best is trial 3 with value: 5.85768985748291.\n",
      "[I 2025-10-20 12:06:43,714] Trial 7 finished with value: 5.872347354888916 and parameters: {'learning_rate': 0.09860673295429234, 'max_depth': 7, 'subsample': 0.8586310150168677, 'colsample_bytree': 0.6243322944212932, 'reg_lambda': 1.6978837821443227}. Best is trial 3 with value: 5.85768985748291.\n",
      "[I 2025-10-20 12:07:57,773] Trial 8 finished with value: 5.8640546798706055 and parameters: {'learning_rate': 0.043785631954699485, 'max_depth': 6, 'subsample': 0.6010414843563721, 'colsample_bytree': 0.7450264869777508, 'reg_lambda': 0.04087913069938357}. Best is trial 3 with value: 5.85768985748291.\n",
      "[I 2025-10-20 12:09:26,281] Trial 9 finished with value: 5.865052700042725 and parameters: {'learning_rate': 0.042373185576874625, 'max_depth': 8, 'subsample': 0.6792216598748874, 'colsample_bytree': 0.8285260834246361, 'reg_lambda': 0.002642624774750154}. Best is trial 3 with value: 5.85768985748291.\n",
      "✅ Best XGB params: {'learning_rate': 0.03667514229823883, 'max_depth': 8, 'subsample': 0.9522645580022315, 'colsample_bytree': 0.9030693986054679, 'reg_lambda': 1.3468587149554854}\n",
      "✅ Best validation MAE: 5.85768985748291\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def objective_xgb(trial):\n",
    "    params = {\n",
    "        'n_estimators': 400,\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "        'max_depth': trial.suggest_int('max_depth', 6, 8),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 10.0, log=True),\n",
    "        'random_state': 42,\n",
    "        'tree_method': 'hist',\n",
    "        'objective': 'reg:squarederror'\n",
    "    }\n",
    "\n",
    "    model = XGBRegressor(**params)\n",
    "\n",
    "\n",
    "    try:\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            eval_metric='mae',\n",
    "            early_stopping_rounds=30,\n",
    "            verbose=False\n",
    "        )\n",
    "    except TypeError:\n",
    "        model.fit(X_train, y_train, verbose=False)\n",
    "\n",
    "    preds = model.predict(X_val)\n",
    "    return mean_absolute_error(y_val, preds)\n",
    "\n",
    "study_xgb = optuna.create_study(direction='minimize')\n",
    "study_xgb.optimize(objective_xgb, n_trials=10, show_progress_bar=True)\n",
    "\n",
    "print(\"✅ Best XGB params:\", study_xgb.best_trial.params)\n",
    "print(\"✅ Best validation MAE:\", study_xgb.best_value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9087ffcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-20 12:09:26,300] A new study created in memory with name: no-name-b9211da7-36e3-497d-8b68-06aa84ab4eb7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a324a97a8a74a42ae91b8eca0d3ade4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-20 12:12:17,475] Trial 0 finished with value: 5.862231985502138 and parameters: {'learning_rate': 0.02684975123917489, 'max_depth': 6, 'min_samples_leaf': 88, 'l2_regularization': 0.0009250444715130107}. Best is trial 0 with value: 5.862231985502138.\n",
      "[I 2025-10-20 12:14:59,726] Trial 1 finished with value: 5.866473502308536 and parameters: {'learning_rate': 0.01617462151244625, 'max_depth': 7, 'min_samples_leaf': 141, 'l2_regularization': 0.8826589181758928}. Best is trial 0 with value: 5.862231985502138.\n",
      "[I 2025-10-20 12:17:10,378] Trial 2 finished with value: 5.865871213553847 and parameters: {'learning_rate': 0.057798050081713216, 'max_depth': 6, 'min_samples_leaf': 71, 'l2_regularization': 0.014360037120844209}. Best is trial 0 with value: 5.862231985502138.\n",
      "[I 2025-10-20 12:19:07,764] Trial 3 finished with value: 5.861211859370273 and parameters: {'learning_rate': 0.06457496444743627, 'max_depth': 8, 'min_samples_leaf': 59, 'l2_regularization': 0.0001148559562452822}. Best is trial 3 with value: 5.861211859370273.\n",
      "[I 2025-10-20 12:21:29,695] Trial 4 finished with value: 5.863139674178479 and parameters: {'learning_rate': 0.036498512037460996, 'max_depth': 7, 'min_samples_leaf': 109, 'l2_regularization': 0.0006651419008787559}. Best is trial 3 with value: 5.861211859370273.\n",
      "[I 2025-10-20 12:23:38,721] Trial 5 finished with value: 5.87055995207555 and parameters: {'learning_rate': 0.08457909232118137, 'max_depth': 6, 'min_samples_leaf': 56, 'l2_regularization': 0.5347195480310415}. Best is trial 3 with value: 5.861211859370273.\n",
      "[I 2025-10-20 12:25:48,621] Trial 6 finished with value: 5.865189383976649 and parameters: {'learning_rate': 0.06595031765038463, 'max_depth': 6, 'min_samples_leaf': 69, 'l2_regularization': 0.0009419249768078855}. Best is trial 3 with value: 5.861211859370273.\n",
      "[I 2025-10-20 12:28:12,700] Trial 7 finished with value: 5.863470132696858 and parameters: {'learning_rate': 0.04517226754807046, 'max_depth': 8, 'min_samples_leaf': 106, 'l2_regularization': 0.0242180759341248}. Best is trial 3 with value: 5.861211859370273.\n",
      "[I 2025-10-20 12:30:39,296] Trial 8 finished with value: 5.8641185742538715 and parameters: {'learning_rate': 0.022773219452902307, 'max_depth': 6, 'min_samples_leaf': 68, 'l2_regularization': 0.003479103953112181}. Best is trial 3 with value: 5.861211859370273.\n",
      "[I 2025-10-20 12:33:12,137] Trial 9 finished with value: 5.866067539717054 and parameters: {'learning_rate': 0.018477683668683496, 'max_depth': 6, 'min_samples_leaf': 75, 'l2_regularization': 0.00043217612024765834}. Best is trial 3 with value: 5.861211859370273.\n",
      "Best HIST params: {'learning_rate': 0.06457496444743627, 'max_depth': 8, 'min_samples_leaf': 59, 'l2_regularization': 0.0001148559562452822}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "def objective_hist(trial):\n",
    "    params = {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "        'max_depth': trial.suggest_int('max_depth', 6, 8),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 20, 200),\n",
    "        'l2_regularization': trial.suggest_float('l2_regularization', 1e-4, 1.0, log=True),\n",
    "        'max_iter': 500,\n",
    "        'random_state': 42\n",
    "    }\n",
    "\n",
    "    model = HistGradientBoostingRegressor(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_val)\n",
    "    return mean_absolute_error(y_val, preds)\n",
    "\n",
    "study_hist = optuna.create_study(direction='minimize')\n",
    "study_hist.optimize(objective_hist, n_trials=10, show_progress_bar=True)\n",
    "print(\"Best HIST params:\", study_hist.best_trial.params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7bd30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.986385487639305, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.986385487639305\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.810473184318834, subsample=1.0 will be ignored. Current value: bagging_fraction=0.810473184318834\n",
      "[LightGBM] [Warning] feature_fraction is set=0.986385487639305, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.986385487639305\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.810473184318834, subsample=1.0 will be ignored. Current value: bagging_fraction=0.810473184318834\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.136063 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 22869\n",
      "[LightGBM] [Info] Number of data points in the train set: 3767984, number of used features: 98\n",
      "[LightGBM] [Info] Start training from score -0.045446\n",
      "[LightGBM] [Warning] feature_fraction is set=0.986385487639305, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.986385487639305\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.810473184318834, subsample=1.0 will be ignored. Current value: bagging_fraction=0.810473184318834\n",
      "[LightGBM] [Warning] feature_fraction is set=0.986385487639305, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.986385487639305\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.810473184318834, subsample=1.0 will be ignored. Current value: bagging_fraction=0.810473184318834\n",
      "[LightGBM] [Warning] feature_fraction is set=0.986385487639305, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.986385487639305\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.810473184318834, subsample=1.0 will be ignored. Current value: bagging_fraction=0.810473184318834\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.139548 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 22868\n",
      "[LightGBM] [Info] Number of data points in the train set: 3767984, number of used features: 98\n",
      "[LightGBM] [Info] Start training from score -0.044862\n",
      "[LightGBM] [Warning] feature_fraction is set=0.986385487639305, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.986385487639305\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.810473184318834, subsample=1.0 will be ignored. Current value: bagging_fraction=0.810473184318834\n",
      "[LightGBM] [Warning] feature_fraction is set=0.986385487639305, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.986385487639305\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.810473184318834, subsample=1.0 will be ignored. Current value: bagging_fraction=0.810473184318834\n",
      "[LightGBM] [Warning] feature_fraction is set=0.986385487639305, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.986385487639305\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.810473184318834, subsample=1.0 will be ignored. Current value: bagging_fraction=0.810473184318834\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.158672 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 22869\n",
      "[LightGBM] [Info] Number of data points in the train set: 3767984, number of used features: 98\n",
      "[LightGBM] [Info] Start training from score -0.047157\n",
      "[LightGBM] [Warning] feature_fraction is set=0.986385487639305, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.986385487639305\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.810473184318834, subsample=1.0 will be ignored. Current value: bagging_fraction=0.810473184318834\n",
      "[LightGBM] [Warning] feature_fraction is set=0.986385487639305, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.986385487639305\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.810473184318834, subsample=1.0 will be ignored. Current value: bagging_fraction=0.810473184318834\n",
      "[LightGBM] [Warning] feature_fraction is set=0.986385487639305, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.986385487639305\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.810473184318834, subsample=1.0 will be ignored. Current value: bagging_fraction=0.810473184318834\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.147904 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 22868\n",
      "[LightGBM] [Info] Number of data points in the train set: 3767984, number of used features: 98\n",
      "[LightGBM] [Info] Start training from score -0.046271\n",
      "[LightGBM] [Warning] feature_fraction is set=0.986385487639305, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.986385487639305\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.810473184318834, subsample=1.0 will be ignored. Current value: bagging_fraction=0.810473184318834\n",
      "[LightGBM] [Warning] feature_fraction is set=0.986385487639305, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.986385487639305\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.810473184318834, subsample=1.0 will be ignored. Current value: bagging_fraction=0.810473184318834\n",
      "[LightGBM] [Warning] feature_fraction is set=0.986385487639305, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.986385487639305\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.810473184318834, subsample=1.0 will be ignored. Current value: bagging_fraction=0.810473184318834\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.157742 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 22869\n",
      "[LightGBM] [Info] Number of data points in the train set: 3767984, number of used features: 98\n",
      "[LightGBM] [Info] Start training from score -0.043522\n",
      "[LightGBM] [Warning] feature_fraction is set=0.986385487639305, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.986385487639305\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.810473184318834, subsample=1.0 will be ignored. Current value: bagging_fraction=0.810473184318834\n",
      "[LightGBM] [Warning] feature_fraction is set=0.986385487639305, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.986385487639305\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.810473184318834, subsample=1.0 will be ignored. Current value: bagging_fraction=0.810473184318834\n",
      "[LightGBM] [Warning] feature_fraction is set=0.986385487639305, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.986385487639305\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.810473184318834, subsample=1.0 will be ignored. Current value: bagging_fraction=0.810473184318834\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.231035 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 22869\n",
      "[LightGBM] [Info] Number of data points in the train set: 4709980, number of used features: 98\n",
      "[LightGBM] [Info] Start training from score -0.045452\n",
      "[LightGBM] [Warning] feature_fraction is set=0.986385487639305, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.986385487639305\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.810473184318834, subsample=1.0 will be ignored. Current value: bagging_fraction=0.810473184318834\n",
      "Stacked MAE: 5.9954185749733515\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import clone\n",
    "\n",
    "\n",
    "\n",
    "def get_oof_preds(models, X, y, folds=5):\n",
    "    oof = {name: np.zeros(len(X)) for name in models}\n",
    "    preds_val = {name: None for name in models}\n",
    "    kf = KFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "    for tr_idx, val_idx in kf.split(X):\n",
    "        X_tr, X_va = X.iloc[tr_idx], X.iloc[val_idx]\n",
    "        y_tr = y.iloc[tr_idx]\n",
    "        for name, model in models.items():\n",
    "            m = clone(model)\n",
    "            m.fit(X_tr, y_tr)\n",
    "            oof[name][val_idx] = m.predict(X_va)\n",
    "    oof_df = pd.DataFrame(oof)\n",
    "    return oof_df\n",
    "\n",
    "\n",
    "base_models = {\n",
    "    \"lgb\": LGBMRegressor(**study_lgb.best_trial.params, n_estimators=1000),\n",
    "    \"xgb\": XGBRegressor(**study_xgb.best_trial.params, n_estimators=1000),\n",
    "    \"hist\": HistGradientBoostingRegressor(**study_hist.best_trial.params)\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "oof_preds = get_oof_preds(base_models, X_train, y_train, folds=5)\n",
    "meta = RidgeCV(alphas=[0.1,1.0,10.0])\n",
    "meta.fit(oof_preds, y_train)\n",
    "# predict on validation\n",
    "val_preds_base = pd.DataFrame({name: model.fit(X_train, y_train).predict(X_val) for name, model in base_models.items()})\n",
    "ensemble_val_pred = meta.predict(val_preds_base)\n",
    "print(\"Stacked MAE:\", mean_absolute_error(y_val, ensemble_val_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fa9437e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['meta_model.pkl']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save your fitted artifacts\n",
    "joblib.dump(stock_stats, \"stock_stats.pkl\")\n",
    "joblib.dump(base_models, \"base_models.pkl\")\n",
    "joblib.dump(meta, \"meta_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eaa278a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_id</th>\n",
       "      <th>date_id</th>\n",
       "      <th>seconds_in_bucket</th>\n",
       "      <th>imbalance_size</th>\n",
       "      <th>imbalance_buy_sell_flag</th>\n",
       "      <th>reference_price</th>\n",
       "      <th>matched_size</th>\n",
       "      <th>far_price</th>\n",
       "      <th>near_price</th>\n",
       "      <th>bid_price</th>\n",
       "      <th>...</th>\n",
       "      <th>liquidity_ratio</th>\n",
       "      <th>bid_pressure</th>\n",
       "      <th>volatility_proxy</th>\n",
       "      <th>volume</th>\n",
       "      <th>size_imbalance</th>\n",
       "      <th>wap_mean_all</th>\n",
       "      <th>wap_std_all</th>\n",
       "      <th>wap_relative_to_market</th>\n",
       "      <th>similar_stock1</th>\n",
       "      <th>similar_stock2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.180603e+06</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999812</td>\n",
       "      <td>13380277.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.807945</td>\n",
       "      <td>0.877170</td>\n",
       "      <td>NaN</td>\n",
       "      <td>69144.531250</td>\n",
       "      <td>7.141326</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>137</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.666039e+05</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.999896</td>\n",
       "      <td>1642214.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999896</td>\n",
       "      <td>...</td>\n",
       "      <td>0.907894</td>\n",
       "      <td>0.135625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23838.128906</td>\n",
       "      <td>0.156905</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>118</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.028799e+05</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.999561</td>\n",
       "      <td>1819368.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999403</td>\n",
       "      <td>...</td>\n",
       "      <td>0.857283</td>\n",
       "      <td>0.666468</td>\n",
       "      <td>NaN</td>\n",
       "      <td>56951.000000</td>\n",
       "      <td>1.998210</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>94</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.191768e+07</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.000171</td>\n",
       "      <td>18389746.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>...</td>\n",
       "      <td>0.606774</td>\n",
       "      <td>0.004830</td>\n",
       "      <td>NaN</td>\n",
       "      <td>481357.312500</td>\n",
       "      <td>0.004853</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>123</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.475500e+05</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.999532</td>\n",
       "      <td>17860614.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999394</td>\n",
       "      <td>...</td>\n",
       "      <td>0.975555</td>\n",
       "      <td>0.974343</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16919.638672</td>\n",
       "      <td>37.976360</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>186</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5237975</th>\n",
       "      <td>195</td>\n",
       "      <td>480</td>\n",
       "      <td>540</td>\n",
       "      <td>2.440723e+06</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.000317</td>\n",
       "      <td>28280362.00</td>\n",
       "      <td>0.999734</td>\n",
       "      <td>0.999734</td>\n",
       "      <td>1.000317</td>\n",
       "      <td>...</td>\n",
       "      <td>0.920552</td>\n",
       "      <td>0.091608</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>352119.437500</td>\n",
       "      <td>0.100847</td>\n",
       "      <td>0.999098</td>\n",
       "      <td>0.002293</td>\n",
       "      <td>1.001231</td>\n",
       "      <td>166</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5237976</th>\n",
       "      <td>196</td>\n",
       "      <td>480</td>\n",
       "      <td>540</td>\n",
       "      <td>3.495105e+05</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.000643</td>\n",
       "      <td>9187699.00</td>\n",
       "      <td>1.000129</td>\n",
       "      <td>1.000386</td>\n",
       "      <td>1.000643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.963353</td>\n",
       "      <td>0.687127</td>\n",
       "      <td>-0.000257</td>\n",
       "      <td>298501.468750</td>\n",
       "      <td>2.196184</td>\n",
       "      <td>0.999098</td>\n",
       "      <td>0.002293</td>\n",
       "      <td>1.001722</td>\n",
       "      <td>182</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5237977</th>\n",
       "      <td>197</td>\n",
       "      <td>480</td>\n",
       "      <td>540</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.995789</td>\n",
       "      <td>12725436.00</td>\n",
       "      <td>0.995789</td>\n",
       "      <td>0.995789</td>\n",
       "      <td>0.995789</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.085306</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>196828.968750</td>\n",
       "      <td>0.093262</td>\n",
       "      <td>0.999098</td>\n",
       "      <td>0.002293</td>\n",
       "      <td>0.996696</td>\n",
       "      <td>194</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5237978</th>\n",
       "      <td>198</td>\n",
       "      <td>480</td>\n",
       "      <td>540</td>\n",
       "      <td>1.000899e+06</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999210</td>\n",
       "      <td>94773272.00</td>\n",
       "      <td>0.999210</td>\n",
       "      <td>0.999210</td>\n",
       "      <td>0.998970</td>\n",
       "      <td>...</td>\n",
       "      <td>0.989549</td>\n",
       "      <td>0.157923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>795524.750000</td>\n",
       "      <td>0.187540</td>\n",
       "      <td>0.999098</td>\n",
       "      <td>0.002293</td>\n",
       "      <td>0.999910</td>\n",
       "      <td>160</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5237979</th>\n",
       "      <td>199</td>\n",
       "      <td>480</td>\n",
       "      <td>540</td>\n",
       "      <td>1.884286e+06</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.002129</td>\n",
       "      <td>24073678.00</td>\n",
       "      <td>1.000859</td>\n",
       "      <td>1.001494</td>\n",
       "      <td>1.002129</td>\n",
       "      <td>...</td>\n",
       "      <td>0.927410</td>\n",
       "      <td>0.454488</td>\n",
       "      <td>-0.000634</td>\n",
       "      <td>550249.000000</td>\n",
       "      <td>0.833139</td>\n",
       "      <td>0.999098</td>\n",
       "      <td>0.002293</td>\n",
       "      <td>1.003178</td>\n",
       "      <td>197</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5237980 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         stock_id  date_id  seconds_in_bucket  imbalance_size  \\\n",
       "0               0        0                  0    3.180603e+06   \n",
       "1               1        0                  0    1.666039e+05   \n",
       "2               2        0                  0    3.028799e+05   \n",
       "3               3        0                  0    1.191768e+07   \n",
       "4               4        0                  0    4.475500e+05   \n",
       "...           ...      ...                ...             ...   \n",
       "5237975       195      480                540    2.440723e+06   \n",
       "5237976       196      480                540    3.495105e+05   \n",
       "5237977       197      480                540    0.000000e+00   \n",
       "5237978       198      480                540    1.000899e+06   \n",
       "5237979       199      480                540    1.884286e+06   \n",
       "\n",
       "         imbalance_buy_sell_flag  reference_price  matched_size  far_price  \\\n",
       "0                              1         0.999812   13380277.00        NaN   \n",
       "1                             -1         0.999896    1642214.25        NaN   \n",
       "2                             -1         0.999561    1819368.00        NaN   \n",
       "3                             -1         1.000171   18389746.00        NaN   \n",
       "4                             -1         0.999532   17860614.00        NaN   \n",
       "...                          ...              ...           ...        ...   \n",
       "5237975                       -1         1.000317   28280362.00   0.999734   \n",
       "5237976                       -1         1.000643    9187699.00   1.000129   \n",
       "5237977                        0         0.995789   12725436.00   0.995789   \n",
       "5237978                        1         0.999210   94773272.00   0.999210   \n",
       "5237979                       -1         1.002129   24073678.00   1.000859   \n",
       "\n",
       "         near_price  bid_price  ...  liquidity_ratio  bid_pressure  \\\n",
       "0               NaN   0.999812  ...         0.807945      0.877170   \n",
       "1               NaN   0.999896  ...         0.907894      0.135625   \n",
       "2               NaN   0.999403  ...         0.857283      0.666468   \n",
       "3               NaN   0.999999  ...         0.606774      0.004830   \n",
       "4               NaN   0.999394  ...         0.975555      0.974343   \n",
       "...             ...        ...  ...              ...           ...   \n",
       "5237975    0.999734   1.000317  ...         0.920552      0.091608   \n",
       "5237976    1.000386   1.000643  ...         0.963353      0.687127   \n",
       "5237977    0.995789   0.995789  ...         1.000000      0.085306   \n",
       "5237978    0.999210   0.998970  ...         0.989549      0.157923   \n",
       "5237979    1.001494   1.002129  ...         0.927410      0.454488   \n",
       "\n",
       "         volatility_proxy         volume  size_imbalance  wap_mean_all  \\\n",
       "0                     NaN   69144.531250        7.141326      1.000000   \n",
       "1                     NaN   23838.128906        0.156905      1.000000   \n",
       "2                     NaN   56951.000000        1.998210      1.000000   \n",
       "3                     NaN  481357.312500        0.004853      1.000000   \n",
       "4                     NaN   16919.638672       37.976360      1.000000   \n",
       "...                   ...            ...             ...           ...   \n",
       "5237975          0.000000  352119.437500        0.100847      0.999098   \n",
       "5237976         -0.000257  298501.468750        2.196184      0.999098   \n",
       "5237977          0.000000  196828.968750        0.093262      0.999098   \n",
       "5237978          0.000000  795524.750000        0.187540      0.999098   \n",
       "5237979         -0.000634  550249.000000        0.833139      0.999098   \n",
       "\n",
       "        wap_std_all  wap_relative_to_market  similar_stock1  similar_stock2  \n",
       "0          0.000000                1.000000             137             186  \n",
       "1          0.000000                1.000000             118             119  \n",
       "2          0.000000                1.000000              94             146  \n",
       "3          0.000000                1.000000             123              37  \n",
       "4          0.000000                1.000000             186              35  \n",
       "...             ...                     ...             ...             ...  \n",
       "5237975    0.002293                1.001231             166             187  \n",
       "5237976    0.002293                1.001722             182              32  \n",
       "5237977    0.002293                0.996696             194             128  \n",
       "5237978    0.002293                0.999910             160             163  \n",
       "5237979    0.002293                1.003178             197             182  \n",
       "\n",
       "[5237980 rows x 32 columns]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def compute_stock_embeddings(df):\n",
    "#     \"\"\"Compute per-stock summary statistics (no future leakage).\"\"\"\n",
    "#     stock_stats = df.groupby('stock_id').agg(\n",
    "#         wap_mean=('wap', 'mean'),\n",
    "#         wap_std=('wap', 'std'),\n",
    "#         wap_skew=('wap', lambda x: x.skew()),\n",
    "#         spread_mean=('spread', 'mean'),\n",
    "#         spread_std=('spread', 'std'),\n",
    "#         matched_mean=('matched_size', 'mean'),\n",
    "#         imbalance_mean=('imbalance_size', 'mean'),\n",
    "#         imbalance_std=('imbalance_size', 'std'),\n",
    "#         buy_sell_flag_mean=('imbalance_buy_sell_flag', 'mean'),\n",
    "#         liquidity_mean=('matched_size', lambda x: x.mean() / (x.mean() + 1e-6))\n",
    "#     ).fillna(0)\n",
    "#     return stock_stats.reset_index()\n",
    "\n",
    "\n",
    "\n",
    "# def fit_kmeans_and_neighbors(stock_stats, n_clusters=20, n_neighbors=6, random_state=42):\n",
    "#     X = stock_stats.drop(columns=['stock_id']).values\n",
    "#     scaler = StandardScaler()\n",
    "#     X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "#     kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init='auto')\n",
    "#     kmeans.fit(X_scaled)\n",
    "\n",
    "#     # nearest neighbor model\n",
    "#     nn = NearestNeighbors(n_neighbors=n_neighbors, metric='euclidean')\n",
    "#     nn.fit(X_scaled)\n",
    "\n",
    "#     # attach results\n",
    "#     stock_stats['cluster'] = kmeans.labels_\n",
    "#     stock_stats['embedding'] = list(X_scaled)\n",
    "#     return scaler, kmeans, nn, stock_stats, X_scaled\n",
    "\n",
    "\n",
    "# def get_similar_and_opposite(stock_stats, X_scaled, n_similar=2):\n",
    "#     stock_ids = stock_stats['stock_id'].values\n",
    "#     dist = pairwise_distances(X_scaled, metric='euclidean')\n",
    "\n",
    "#     results = []\n",
    "#     for i, sid in enumerate(stock_ids):\n",
    "#         order = np.argsort(dist[i])\n",
    "#         similar_ids = stock_ids[order[1:n_similar+1]]  # skip self (index 0)\n",
    "        \n",
    "#         # create dictionary with dynamic keys\n",
    "#         row = {'stock_id': sid}\n",
    "#         for j, sim_id in enumerate(similar_ids):\n",
    "#             row[f'similar_stock{j+1}'] = sim_id\n",
    "#         results.append(row)\n",
    "    \n",
    "#     # convert list of dicts → DataFrame\n",
    "#     return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "\n",
    "# # 1. Compute stock-level embeddings\n",
    "# stock_stats = compute_stock_embeddings(engineered_df)   # train_df = your training data DataFrame\n",
    "\n",
    "# # 2. Fit clustering and neighbors\n",
    "# scaler, kmeans, nn, stock_stats, X_scaled = fit_kmeans_and_neighbors(stock_stats,\n",
    "#                                                                      n_clusters=20,\n",
    "#                                                                      n_neighbors=6,\n",
    "#                                                                      random_state=42)\n",
    "\n",
    "# # 3. Get similar and opposite stock IDs for each stock\n",
    "# pairs_df = get_similar_and_opposite(stock_stats, X_scaled,\n",
    "#                                     n_similar=2)\n",
    "\n",
    "\n",
    "# df_with_neighbors = engineered_df.merge(pairs_df, on='stock_id', how='left')\n",
    "# df_with_neighbors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c78424",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
