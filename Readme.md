# Optiver Kaggle Competition 2023 ğŸ§ ğŸ“ˆ
My solution for the [Optiver Trading at the Close (2023)](https://www.kaggle.com/competitions/optiver-trading-at-the-close/overview) Kaggle competition.  
The competition focuses on predicting the **target value** for financial time series data at the close of trading, based on order book and trade features.

---

## ğŸ“‚ Competition Overview
The dataset and competition details are available here:  
ğŸ”— [Competition Page](https://www.kaggle.com/competitions/optiver-trading-at-the-close/overview)  
ğŸ”— [Dataset](https://www.kaggle.com/competitions/optiver-trading-at-the-close/data)

The goal is to build a model that predicts **future price movements** using limit order book data for multiple stocks across different time intervals.

---

## ğŸ§° Project Structure

---

### âš™ï¸ Data Preprocessing Pipeline

1. **Feature Engineering**
   - Created rolling statistics of WAP (weighted average price), spread, and volatility at stock level.
   - Derived imbalance ratios, matched sizes, and directional indicators.
   - Aggregated stock-level statistics such as mean, standard deviation, and skewness.

2. **Missing Value Handling**
   - Numerical columns filled using **median imputation**.
   - Group-level imputation based on `stock_id` where appropriate.

3. **Normalization**
   - Standard scaling applied to features with different magnitudes for better model convergence.

4. **Trainâ€“Validation Split**
   - The training data was split **90% train / 10% validation** to evaluate performance locally.

---

### ğŸ¤– Models Used

| Model | Library | Key Hyperparameters |
|--------|----------|--------------------|
| LightGBM | `lightgbm.LGBMRegressor` | `n_estimators=500`, `learning_rate=0.05`, `num_leaves=31` |
| XGBoost | `xgboost.XGBRegressor` | `n_estimators=500`, `learning_rate=0.05`, `max_depth=6` |
| HistGradientBoosting | `sklearn.ensemble.HistGradientBoostingRegressor` | `max_iter=500`, `learning_rate=0.05` |

Each base model was trained on the processed features and evaluated using **Mean Absolute Error (MAE)**.

---

### ğŸ§© Ensemble Model (Stacking)

The predictions from the three base models were combined using a **meta-model (Linear Regression)** trained on out-of-fold predictions.  
This stacking approach improved the stability and accuracy of the final predictions.

---

### ğŸ“Š Evaluation

- Metric: **Mean Absolute Error (MAE)**
- Validation Strategy: Time-aware split based on `date_id` and `time_id`
- Observed improvement: Ensemble model reduced MAE by ~3â€“5% compared to individual models.

---



